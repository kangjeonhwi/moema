# scripts/run_continued_pretraining.py
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling

from src.model.modeling_moema import MoemaForCausalLM 

def print_trainable_parameters(model):
    trainable_params, all_param = 0, 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || "
        f"trainable%: {100 * trainable_params / all_param:.2f}"
    )

def main():
    print("Llama loading...")
    llama_path = "meta-llama/Llama-3.2-1B" # Base llama
    DTYPE = torch.bfloat16

    print("Model processing...")
    model = MoemaForCausalLM.from_llama(
        model_path=llama_path,
        num_experts=8,
        num_experts_per_tok=2,
        torch_dtype=DTYPE,
    )

    tokenizer = AutoTokenizer.from_pretrained(llama_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("\nWeight Freezing...")
    for param in model.parameters():
        param.requires_grad = False

    trainable_layer_keywords = ["block_sparse_moe", "gate"]
    for name, param in model.named_parameters():
        if any(keyword in name for keyword in trainable_layer_keywords):
            param.requires_grad = True

    print("\nâœ… ìµœì¢… í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° í˜„í™©:")
    print_trainable_parameters(model)


    # --- 3. ë°ì´í„°ì…‹ ì¤€ë¹„ (Continued Pre-trainingìš©) ---
    # Instruction í˜•ì‹ì´ ì•„ë‹Œ, ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    train_file = "dataset/train/llama_finetune_2048.jsonl" # íŒŒì¼ëª…ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    dataset = load_dataset("json", data_files=train_file, split="train")

    # í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í† í° IDë¡œ ë³€í™˜í•˜ëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, max_length=2048)
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])


    # --- 4. í•™ìŠµ ì„¤ì • (TrainingArguments) ---
    training_args = TrainingArguments(
        output_dir="./outputs/domain_pretrain",
        per_device_train_batch_size=2, # GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ì¡°ì •
        gradient_accumulation_steps=4,
        learning_rate=5e-5, # ì‚¬ì „í•™ìŠµì€ ì¼ë°˜ì ìœ¼ë¡œ ë” ë‚®ì€ í•™ìŠµë¥ ì„ ì‚¬ìš©
        num_train_epochs=1,
        save_strategy="epoch",
        logging_steps=10,
        bf16=True,
        report_to="tensorboard",
    )
    
    # --- 5. ê¸°ë³¸ 'Trainer' ìƒì„± ë° í•™ìŠµ ì‹œì‘ ---
    # SFTTrainer ëŒ€ì‹  ê¸°ë³¸ Trainerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # DataCollatorForLanguageModelingì€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ í•™ìŠµí•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ ë¬¶ì–´ì¤ë‹ˆë‹¤.
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    print("\nğŸš€ ë„ë©”ì¸ ì ì‘ ì‚¬ì „í•™ìŠµ(Continued Pre-training)ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    trainer.train()

    # --- 6. í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ---
    final_model_path = "./outputs/domain_pretrain/final_model"
    trainer.save_model(final_model_path)
    print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ! ë²•ë¥  ì§€ì‹ì´ ì£¼ì…ëœ ëª¨ë¸ì´ {final_model_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
